{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63b644c0",
   "metadata": {},
   "source": [
    "# Estimating Internal Migration\n",
    "\n",
    "This notebook accompanies the paper ***Using Twitter to track internal migration before and during the COVID-19 pandemic in the UK*** by Yikang Wang, Chen Zhong and Carmen Cabrera-Arnau.\n",
    "\n",
    "Code segments have been simplified to exclude database queries previously integrated within the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec1ac5c",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5707e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from shapely import geometry\n",
    "import os\n",
    "import requests\n",
    "import math\n",
    "import re\n",
    "import socket\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91125a3",
   "metadata": {},
   "source": [
    "## Twitter Geocoding\n",
    "\n",
    "### Lookup Table\n",
    "\n",
    "Matching Twitter place attributes to officially defined UK administrative divisions by a loopup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd917396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some unofficial place names in the Twitter place attributes\n",
    "lookup_dict = {\n",
    "    'Bristol': 'Bristol, City of',\n",
    "    'Dundee': 'Dundee City',\n",
    "    'Edinburgh': 'City of Edinburgh',\n",
    "    'Glasgow': 'Glasgow City',\n",
    "    'Kingston upon Hull': 'Kingston upon Hull, City of',\n",
    "    'Herefordshire': 'Herefordshire, County of',\n",
    "    'North East': 'North East (England)',\n",
    "    'North West': 'North West (England)',\n",
    "    'South East': 'South East (England)',\n",
    "    'South West': 'South West (England)',\n",
    "    'East Midlands': 'East Midlands (England)',\n",
    "    'Saint Helier': 'Jersey',\n",
    "    'Sale': 'Trafford',\n",
    "    'Huddersfield': 'Kirklees',\n",
    "    'West Bromwich': 'Sandwell',\n",
    "    'Ashton-under-Lyne': 'Tameside'\n",
    "}\n",
    "\n",
    "# UK LAD to LAU to ITL lookup data\n",
    "# Data source: https://geoportal.statistics.gov.uk/documents/1191f4fc8e06433b9196103eac198d56/about\n",
    "lookup_df = pd.read_csv('LAD21_LAU121_ITL321_ITL221_ITL121_UK_LU.csv')\n",
    "\n",
    "# Set of places with a scale greater than the LAD scale\n",
    "larger_than_LAD_set = set.union(set(lookup_df.LAU121NM),\n",
    "                                set(lookup_df.ITL321NM),\n",
    "                                set(lookup_df.ITL221NM),\n",
    "                                set(lookup_df.ITL121NM)) - LAD_set\n",
    "\n",
    "# Split LAD names to bilud places to LAD lookup table\n",
    "LAD_set = set(lookup_df.LAD21NM)\n",
    "LAD_split_dict = {}\n",
    "for loc in LAD_set:\n",
    "    if len(loc.split(' and ')) >= 2 and len(re.split(' and |, ', loc)) >= 2:\n",
    "        for i in range(len(re.split(' and |, ', loc))):\n",
    "            LAD_split_dict.update({re.split(' and |, ', loc)[i]: loc})\n",
    "lookup_dict.update(LAD_split_dict)\n",
    "\n",
    "# UK ward to LAD lookup data\n",
    "# Data source: https://geoportal.statistics.gov.uk/documents/ward-to-local-authority-district-december-2021-lookup-in-the-united-kingdom/about\n",
    "ward_dict = pd.read_csv('WD21_LAD21_UK_LU_provisional.csv').set_index('WD21NM').LAD21NM.to_dict()\n",
    "lookup_dict.update(ward_dict)\n",
    "\n",
    "# Split ward names to bilud places to LAD lookup table\n",
    "ward_split_dict = {}\n",
    "for loc in ward_dict:\n",
    "    if len(loc.split(' and ')) >= 2 and len(re.split(' and |, ', loc)) >= 2:\n",
    "        for i in range(len(re.split(' and |, ', loc))):\n",
    "            ward_split_dict.update(\n",
    "                {re.split(' and |, ', loc)[i]: ward_dict[loc]})\n",
    "lookup_dict.update(ward_split_dict)\n",
    "\n",
    "# Delete duplicate place names\n",
    "for i in (lookup_dict).copy():\n",
    "    if i in LAD_set or i in larger_than_LAD_set:\n",
    "        del lookup_dict[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4358697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match place names to administrative divisions using the lookup table\n",
    "def match(location):\n",
    "    if location in LAD_set:\n",
    "        return 1  # matched on LAD scale\n",
    "    elif location in larger_than_LAD_set:\n",
    "        return 2  # matched on a scale greater than LAD\n",
    "    elif location in lookup_dict:\n",
    "        return match(lookup_dict[location])\n",
    "    return 0  # not matched\n",
    "\n",
    "\n",
    "# Change unofficial place names to official names using the lookup table\n",
    "def change_place_name(place):\n",
    "    if place in lookup_dict:\n",
    "        return lookup_dict[place]\n",
    "    elif match(place) > 0:\n",
    "        return place\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2c159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Example '''\n",
    "# Read Twitter place attributes data\n",
    "df = pd.read_csv('twitter_place_attributes.csv')\n",
    "\n",
    "# Change place names and match with the lookup table\n",
    "df['locality'] = df['location'].apply(change_place_name)\n",
    "df['locatity_match'] = df['locality'].apply(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14bb484",
   "metadata": {},
   "source": [
    "### Bing Map API Geocoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaa123b",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_key = ''  # Bing Map Geocoding API key\n",
    "\n",
    "for i in tqdm(range(df.shape[0])):\n",
    "    # If the place is already matched with the lookup table, then skip API geocoding\n",
    "    if df.loc[i, 'locality'] != '':\n",
    "        continue\n",
    "\n",
    "    place = df.iloc[i, 0]\n",
    "    URL = \"http://dev.virtualearth.net/REST/v1/Locations?q=\" + place + \"%20UK&o=json&key=\" + API_key\n",
    "    response = requests.get(URL)\n",
    "    if response.status_code == 200:\n",
    "        if response.json()['resourceSets'][0]['estimatedTotal'] == 0:\n",
    "            continue\n",
    "        address = response.json()['resourceSets'][0]['resources'][0]['address']\n",
    "\n",
    "        # If the returned locality is not matched with the lookup table, try to match the returned address\n",
    "        try:\n",
    "            locality = address['locality']\n",
    "            address_num = 0\n",
    "        except:\n",
    "            locality = address['formattedAddress'].split(', ')[0]\n",
    "            address_num = 1\n",
    "        if match(locality) == 0:\n",
    "            for j in range(address_num, len(address['formattedAddress'].split(', '))):\n",
    "                locality = address['formattedAddress'].split(', ')[j]\n",
    "                if match(locality) > 0:\n",
    "                    break\n",
    "\n",
    "        # If the returned locality is not matched on the LAD scale, save the bounding box\n",
    "        if match(locality) != 1:\n",
    "            df.loc[i, 'bbox0'], df.loc[i, 'bbox1'], df.loc[i, 'bbox2'], df.loc[i, 'bbox3'] = response.json()['resourceSets'][0]['resources'][0]['bbox']\n",
    "\n",
    "        # Change to official locality names\n",
    "        if locality in special_loc_dict:\n",
    "            df.loc[i, 'locality'] = special_loc_dict[locality]\n",
    "        else:\n",
    "            df.loc[i, 'locality'] = locality\n",
    "\n",
    "# Match with the lookup table\n",
    "df['locality_match'] = df['locality'].apply(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2911ec3c",
   "metadata": {},
   "source": [
    "### Bounding Box Intersection Geocoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02845c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IOU(se):\n",
    "    try:\n",
    "        bboxlist = list(se)\n",
    "        y1, x1, y2, x2 = bboxlist[0], bboxlist[1], bboxlist[2], bboxlist[3]\n",
    "        if np.isnan(y1) or y2 - y1 > 2:\n",
    "            return ''\n",
    "        poly2 = geometry.box(x1, y1, x2, y2)\n",
    "        inter_se = LAD['geometry'].intersects(poly2)\n",
    "        temp_df = LAD[inter_se]\n",
    "        IOU_se = temp_df['geometry'].intersection(poly2).area / poly2.area\n",
    "        if IOU_se.max() > 0.6:\n",
    "            return LAD.loc[IOU_se.idxmax(), 'LAD21NM']\n",
    "    except:\n",
    "        return ''\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c675a135",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Example '''\n",
    "# Read LAD geo data\n",
    "# Data source: https://geoportal.statistics.gov.uk/maps/local-authority-districts-december-2021-uk-bgc\n",
    "LAD = gp.read_file('Local_Authority_Districts_(May_2021)_UK_BGC.geojson')\n",
    "\n",
    "# IOU geocoding\n",
    "df.loc[:, 'locality_IOU'] = df[['bbox0', 'bbox1', 'bbox2', 'bbox3']].apply(IOU, axis=1)\n",
    "\n",
    "# Match IOU geocoding result with the lookup table\n",
    "df['locality_IOU_match'] = df['locality_IOU'].apply(match)\n",
    "\n",
    "# If the IOU geocoding is successful, update the locality\n",
    "df.loc[df['locality_IOU_match'] == 1, 'locality'] = df.loc[df['locality_IOU_match'] == 1, 'locality_IOU']\n",
    "df['locality_match'] = df['locality'].apply(match)\n",
    "\n",
    "# Save result\n",
    "df.to_csv('twitter_place_attributes_LAD.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ae083a",
   "metadata": {},
   "source": [
    "## User Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7ce0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_localization(userid):\n",
    "    temp_df = twitter_LAD_count_df[twitter_LAD_count_df.userid == userid]\n",
    "    if temp_df.counts.sum() > 1 and temp_df.counts.max()/temp_df.counts.sum() > 0.65:\n",
    "        return temp_df.locality.iloc[0]\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdf276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Example '''\n",
    "# Read Twitter place geocoding result\n",
    "twitter_place_LAD = pd.read_csv('twitter_place_attributes_LAD.csv')[['location', 'locality', 'locality_match']]\n",
    "loc_LAD = loc_LAD[loc_LAD.locality_match == 1].reset_index(drop=True)\n",
    "loc_LAD = loc_LAD[['location', 'locality']]\n",
    "\n",
    "for year in [2019, 2020, 2021]:\n",
    "    for month in range(13):\n",
    "        # Read monthly Twitter data\n",
    "        twitter_df = pd.read_csv('data/uk' + str(year) + '-' + str(month).zfill(2) + '.csv')\n",
    "        twitter_LAD.userid = twitter_LAD.userid.apply(lambda x: round(float(x)))  # avoid string type data\n",
    "        twitter_df.userid = twitter_df.userid.astype('int')\n",
    "\n",
    "        # Match with Twitter geocoding result\n",
    "        twitter_LAD = twitter_df.merge(loc_LAD, how='left', on='location')[['userid', 'locality']]\n",
    "        twitter_LAD = twitter_LAD[twitter_LAD.locality.isna() == False]\n",
    "\n",
    "        user_LAD = pd.DataFrame(twitter_LAD.userid.value_counts()).reset_index()\n",
    "        user_LAD.columns = ['userid', 'counts']\n",
    "        user_LAD = user_LAD[user_LAD.counts > 1]\n",
    "        twitter_LAD = twitter_LAD[['userid', 'locality']]\n",
    "        twitter_LAD_count_df = twitter_LAD.value_counts().reset_index()\n",
    "        twitter_LAD_count_df.columns = ['userid', 'locality', 'counts']\n",
    "\n",
    "        # If all Tweet from the user came from the same LAD, use it as the user's location\n",
    "        user_loc_pair = twitter_LAD.drop_duplicates()\n",
    "        user_loc_count = user_loc_pair.userid.value_counts()\n",
    "        user_oneloc = user_loc_count[user_loc_count == 1].reset_index()\n",
    "        user_oneloc.columns = ['userid', 'oneloc']\n",
    "        user_oneloc = user_oneloc.merge(user_loc_pair, how='left', on='userid')[['userid', 'locality']]\n",
    "        user_oneloc.columns = ['userid', 'home']\n",
    "        user_LAD = user_LAD.merge(user_oneloc, how='left', on='userid')\n",
    "\n",
    "        # For other users, if at least 65% of Tweet from one LAD, use it as the user's location\n",
    "        endline = user_LAD.shape[0] - user_LAD[user_LAD.counts == 2].shape[0]\n",
    "        for i in range(endline):\n",
    "            if type(user_LAD.loc[i, 'home']) == str:\n",
    "                continue\n",
    "            user_LAD.loc[i, 'home'] = user_localization(user_LAD.loc[i, 'userid'])\n",
    "\n",
    "        # Generate a user-home csv file for each month. contains userid and estimated home location\n",
    "        user_LAD.to_csv('data/uk' + str(year) + '-' + str(month).zfill(2) + '-userhome.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c7041f",
   "metadata": {},
   "source": [
    "## User Migration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7101659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def migration(year1, month1, year2, month2):\n",
    "    '''\n",
    "    Compare home location between two months to identify migrations\n",
    "    '''\n",
    "\n",
    "    # Read user-home csv files of these two months\n",
    "    df1 = pd.read_csv('data/uk' + str(year1) + '-' + str(month1).zfill(2) + '-userhome.csv')\n",
    "    df2 = pd.read_csv('data/uk' + str(year2) + '-' + str(month2).zfill(2) + '-userhome.csv')\n",
    "\n",
    "    # Merge user-home csv files\n",
    "    df = df1.merge(df2, how='left', on='userid')\n",
    "    df = df[(df.home_x.isna() == False) & (df.home_y.isna() == False)]\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # migration dataframe\n",
    "    migration_df = df[df.home_x != df.home_y]\n",
    "    \n",
    "    # migration flow dataframe\n",
    "    flow_df = migration_df.groupby(['home_x','home_y']).count().reset_index().iloc[:,:3]\n",
    "    flow_df.columns = ['origin_LAD','destination_LAD','flow']\n",
    "    flow_df['month_start'] = str(year1) + '-' + str(month1).zfill(2)\n",
    "    flow_df['month_end'] = str(year2) + '-' + str(month2).zfill(2)\n",
    "\n",
    "    return flow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66621e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Example '''\n",
    "# Migration flows between mid-2019 and mid-2020\n",
    "df = migration(2019, 6, 2020, 6)\n",
    "df.to_csv('migration_flow_2019_06_2020_06.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e554e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
